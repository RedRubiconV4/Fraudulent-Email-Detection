{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/content/Phishing_Email.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Email_Text'] = dataset['Email Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "dataset['Email_Type'] = label_encoder.fit_transform(dataset['Email Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(dataset['Email_Text'], dataset['Email_Type'], test_size=0.3, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_train_dataset = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "tokenize_val_dataset = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "tokenize_test_dataset = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToEmailDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ConvertToEmailDataset(tokenize_train_dataset, train_labels.tolist())\n",
    "val_dataset = ConvertToEmailDataset(tokenize_val_dataset, val_labels.tolist())\n",
    "test_dataset = ConvertToEmailDataset(tokenize_test_dataset, test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(dataset['Email_Type'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='/content/results',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/content/logs',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "BertModel = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertModel.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = BertModel.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = BertModel.evaluate(test_dataset)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertModel.save_model('/content/BertModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = ['new book japanese linguistics japanese linguistics hamano shoko the george washington university the sound symbolic system of japanese isbn 1 57586 144 5 paper 1 57586 144 5 cloth csli publications 1998 http csli www stanford edu publications email pubs roslin stanford edu this book is the first theoretical study of sound symbolic expressions in japanese commonly known as mimetic words it identifies stringent linguistic constraints on these expressions and demonstrates that they form an intricate linguistic system rather than a collection of ad hoc expressions it then carefully identifies the sound symbolic meanings of sound units so as to make the elusive meaning of each sound symbolic expression fully comprehensible in addition this book describes a number of interesting facts about the history of the japanese language which mimetic words reveal csli publications ventura hall stanford university stanford ca 94305 4115 telephone 650 723 1839 fax 650 725 2166 http csli www stanford edu publications',\n",
    "             'hot stock tip your broker won t share now that oi and gas has entered a long term bul market our speciaity in pinpointing the hottest companies of the few remaining undervaiued energy piays has produced soaring returns montana oi and gas inc mogi to expiore further opportunities in alberta canada a is an energy developer in canada s most highly coveted reservoirs with generating potentia of mi ions per week symbo mogi price 47 increased 11 last three day rating strongbuy how much it wiil up again the vaiue of mogi s shares wil skyrocket 1 price charts confirm oi prices are experiencing the strongest bul market in a generation 2 natural gas prices have tripled in the ast two years 3 with multiple projects in high gear and the expanding production on reserves worth muiti miilions mogi is seiling for ess than 1 4 the vaiue of its assets 4 montana oil and gas specializes in using new technoiogy to turn unproductive oil and gas deposits into profitable enterprises aiready shares in the oil and gas sector are rising faster than the overa market']\n",
    "\n",
    "new_text_tokenize = tokenizer(new_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "class ConvertToPredictDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "new_dataset = ConvertToPredictDataset(new_text_tokenize)\n",
    "predictions = BertModel.predict(new_dataset)\n",
    "\n",
    "predicted_labels = label_encoder.inverse_transform(predictions.predictions.argmax(-1))\n",
    "print(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
